---
title: "BMI713 Problem Set 4"
output:
  html_document: default
  pdf_document: default
header-includes: \usepackage{amsmath}
---
Ivan Sanchez Fernandez




```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE , eval=TRUE)
```

#### Instructions:

Please submit this problem set before class on Tuesday, November 21. Problem sets may be submitted within a week past the due date at a 20% penalty; each person is allowed to submit one problem late (within a week) without penalty. Please include comments in your code to show your work and to clearly indicate your answers. Commented code tends to receive more partial credit!

If you have any questions, please post on the piazza site. This problem set was prepared by Eric Bartell and Jacob Luber, so they will be most prepared to answer questions.

## 1. Correlations (40 points)
```{r, echo=F, eval=F}
# Q1 dataset borrowed from http://www2.stetson.edu/~jrasp/data.htm
# Q2 question inspired by 2016 pset 7
```
Load in the US census population data posted on piazza under the resources tab. The first column is the year the census was taken and second column is the US population. We want to investigate how the passage of time is correlated with the US population.
```{r}
# Load the database and check that it looks reasonable
USPOP <- read.csv("G:\\BMI713 Computational Statistics for Biomedical Sciences\\Homework4\\USPop.csv")
dim(USPOP)
USPOP[1:10, ]
```




### (a) Pearson (10 points)
Calculate the Pearson correlation coefficient. This should be coded "by hand" (use the formulation from lecture; do not use the cor() function). 
Report r between time and US population.
The formula for the Pearson's correlation coefficient is
$$ r = \dfrac{1}{(n - 1)} \sum (\dfrac{x_i - \bar{x}}{s_x}) (\dfrac{y_i - \bar{y}}{s_y})$$
```{r}
# Calculate 1 / (n - 1)
initialterm <- 1 / (dim(USPOP)[1] - 1)

# Calculate the sum with a loop that goes from 1 to n
valuesum <- vector()
for (i in 1 : dim(USPOP)[1]) {
  valuefori <- ((USPOP$Year[i] - mean(USPOP$Year)) / sd(USPOP$Year)) * ((USPOP$Population[i] - mean(USPOP$Population)) / sd(USPOP$Population))
  valuesum <- c(valuesum, valuefori) 
}
finalsum <- sum(valuesum)

# Calculate the Pearson correlation coefficient
r <- initialterm * finalsum
r

# The Pearson correlation coefficient between year and US population is 0.9585941

# Which is the same as from the built-in method
cor(USPOP$Year, USPOP$Population, method = "pearson")
```




### (b) P-value (10 points)
Calculate p-value using both the test statistic t and Fisher's Z-transformation, using the formulas described in class. What do these  p-values tell us?
The formula for the t statistic is
$$ t = r \sqrt{\dfrac{n - 2}{1 - r^2}}$$
```{r}
# Calculate t
t <- r * sqrt((dim(USPOP)[1] - 2) / (1 - r^2))

# For a t statistic with n - 2 degrees of freedom we have
onesided <- pt(t, dim(USPOP)[1] - 2, lower.tail = FALSE)
p <- onesided * 2
p
 
# The p-value is 6.265315e-13

# Which is the same as from the built-in method
cor.test(USPOP$Year, USPOP$Population, method = "pearson")
```

The formula for the Fisher's Z transformation is
$$ Z_r = \dfrac{1}{2} ln\dfrac{r + 1}{1 - r} $$
```{r}
# Calculate Fisher's Z
Z <- (1 / 2) * log((r + 1) / (1 - r))

# For a Z statistic with standard error 1 / sqrt(n - 3) we have
onesided <- pnorm(Z, mean = 0, sd = 1 / sqrt(dim(USPOP)[1] - 3), lower.tail = FALSE)
p <- onesided * 2
p

# The p-value is 6.492181e-18

# Which is significant but different from the built-in method but shows that the Fisher's Z is just an approximation
cor.test(USPOP$Year, USPOP$Population, method = "pearson")
```

```{r}
# These p-values tell us that, with an alpha level of 0.05, the null hypothesis that the variables year and population are not linearly correlated can be rejected.
```




### (c) Spearman (10 points)
Calculate r using the spearman correlation (you do not need to do this by hand). Comment on the difference between correlation coefficients.
```{r}
# Calculate Spearman's correlation coefficient
cor.test(USPOP$Year, USPOP$Population, method = "spearman")

# Spearman correlation coefficient gives a coefficient of 1. This is probably explained because the values do not fit an exact line and, therefore, the Pearson correlation coefficient is slightly less than 1 but the ranks of the values are exactly concordant and, therefore, the correlation of the ranks is 1
# This can be shown graphing the data
plot(USPOP$Year, USPOP$Population)
# The data do not follow a perfect straight line, but the ranks are perfectly correlated
```




### (d) Add an outlier. (10 points)
The data from 2010 got corrupted, and the replacement value is negative the original value! Set the 2010 value to -1*(old value), and re-perform both Pearson and Spearman correlations (you may use built in functions). Comment on the different outputs.
```{r}
# Create the corrupted census and check that it looks as expected
corruptedCensus <- USPOP
corruptedCensus$Population[corruptedCensus$Year == 2010] <- -corruptedCensus$Population[corruptedCensus$Year == 2010]

# Calculate Pearson's correlation coefficient
cor.test(corruptedCensus$Year, corruptedCensus$Population, method = "pearson")

# Calculate Spearman's correlation coefficient
cor.test(corruptedCensus$Year, corruptedCensus$Population, method = "spearman")

# The value for Pearson's correlation coefficient changes dramatically (from the original 0.9585941 to the current 0.3898808) while the Spearman's correlation coefficient changes much less (from the original 1 to the current 0.75). This shows that Spearman's correlation coefficient based on ranks is less susceptible to outliers than Pearson's correlation coefficient based on actual values
```








## 2. Basic Linear Regression (60 points)
We will perform linear regression on the heights of father-son pairs. Use the following code to load the dataset.
The first column is fheight for father's height, and the second column is sheight for son's height.
```{r}
# Load the database and check that it looks as expected
# install.packages("UsingR")
library(UsingR)
data(father.son)
father.son[1:10, ]
```




### (a) Plot the data (5 points)
On a scatterplot, plot father height vs son height. Does the data appear to be roughly linearly correlated?
```{r}
# Plot the data with father height in the X axis and son height in the Y axis
plot(father.son$fheight, father.son$sheight)

# Or with ggplot2
# install.packages("ggplot2")
library(ggplot2)
ggplot(data= father.son, aes(x = fheight, y = sheight)) + geom_point()

# The data appear to be roughly linearly correlated
```




### (b) Least squares (10 points)
Calculate the least squares line with father height as the independent variable manually.
Comment your code. You do not need to perform any calculus; only calculate the slope and intercept.
The formula for the least squares line slope is
$$ b = \dfrac{\sum (x_i - \bar{x}) (y_i - \bar{y})}{\sum (x_i - \bar{x})^2} $$
```{r}
# Calculate the least squares line slope
vectorofvaluesnumerator <- vector()
vectorofvaluesdenominator <- vector()
for (i in 1 : dim(father.son)[1]) {
  
  valueforinumerator <- (father.son$fheight[i] - mean(father.son$fheight)) * (father.son$sheight[i] - mean(father.son$sheight)) 
    
  valueforidenominator <-(father.son$fheight[i] - mean(father.son$fheight)) ^ 2
  
  vectorofvaluesnumerator <- c(vectorofvaluesnumerator, valueforinumerator)
  
  vectorofvaluesdenominator <- c(vectorofvaluesdenominator, valueforidenominator)
}
b <- sum(vectorofvaluesnumerator) / sum(vectorofvaluesdenominator)
b
```

The formula for calculating the least squares line intercept is
$$ a = \bar{y} - b\bar{x} $$
```{r}
# Calculate the least squares line intercept
a <- mean(father.son$sheight) - (b * mean(father.son$fheight))
a

# Therefore, the linear equation would be 
# sonheight = a + b * fatherheight 
# sonheight = 33.8866 + 0.514093 * fatherheight 
```

### (c) Built in (5 points)
Calculate the least squares line using the built in function, and verify that your answer to part (b) is correct.
```{r}
# Linear model
lsrl <- lm(sheight ~ fheight, data = father.son)
lsrl
summary(lsrl)

# The answer to part b) is correct
```




### (d) Plot 2 (5 points)
Add your least squares line to your scatterplot in part (a).
```{r}
# Plot the data with father height in the X axis and son height in the Y axis and the linear regression line
plot(father.son$fheight, father.son$sheight)
abline(lsrl$coefficients[1], lsrl$coefficients[2])

# Or with ggplot2
ggplot(data= father.son, aes(x = fheight, y = sheight)) + geom_point() + geom_smooth(method = "lm", formula = y ~ x)
```





### (e) R^2 (5 points)
How well does your line fit the data? Calculate r^2 (manually).
The formula for calculating the R square is
$$ R^2 = \dfrac{\sum(\hat{y_i} - \bar{y})^2} {\sum(y_i - \bar{y})^2} $$
```{r}
# Calculate R squared
numerator <- vector()
denominator <- vector()

for (i in 1 : dim(father.son)[1]) {
  
  numeratorfori <- (predict(lsrl, newdata = father.son)[i] - mean(father.son$sheight)) ^ 2
  
  denominatorfori <- (father.son$sheight[i] - mean(father.son$sheight)) ^ 2
  
  numerator <- c(numerator, numeratorfori)
  
  denominator <- c(denominator, denominatorfori)
}

R2 <- sum(numerator) / sum(denominator)

R2

# R squared is 0.2513401

# Which is the same as the R squared calculated with the built-in method
summary(lsrl)
```




### (f) Residuals (5 points)
Plot the residuals for different values of x, and comment on their distribution.
```{r}
# Plot the residuals
plot(father.son$fheight, resid(lsrl), 
     ylab= "Residuals", xlab = "Fitted values", 
     main= "Residuals vs Fitted") 
abline(0, 0)       

# The distribution of the residuals follows a distribution with mean 0 and approximately equal variance. The narrower variance at the extreme values of x is explained just because there are fewer values and tend to fall within the central parts of the distribution. In summary, the residuals show that there are no major departures from the homoscedaticity assumption

# A more complete evaluation includes plots for residuals vs fitted, normal quantile-quantile, scale-location, and residuals vs leverage
par(mfrow = c(2, 2))
plot(lsrl)

# These plots also suggest no major departures from the linear model assumptions. The residuals vs leverage plot shows that the points with more leverage (in the extreme of the x variable) have residuals with minor departures from 0 (no major outliers)
```




### (g) Significance (10 points)
How significant is our slope? Calculate both the t-stat and the p-value, and comment on the correlation's significance.
The formula for calculating the standard error of b is
$$ se(b) = \sqrt{\dfrac {(1 / (n - 2)) \sum(y_i - \hat{y_i})^2} {\sum(x_i - \hat{x_i})^2}} $$
```{r}
# Calculate se(b)
firstpartofnumerator <- 1 / (dim(father.son)[1] - 2)

numerator <- vector()
denominator <- vector()

for (i in 1 : dim(father.son)[1]) {
  
  numeratorfori <- (father.son$sheight[i] - predict(lsrl, newdata = father.son)[i]) ^ 2
  
  denominatorfori <- (father.son$fheight[i] - mean(father.son$fheight)) ^ 2
  
  numerator <- c(numerator, numeratorfori)
  
  denominator <- c(denominator, denominatorfori)
}


SEb <- sqrt((firstpartofnumerator * sum(numerator)) / sum(denominator)) 
SEb
```

The formula for calculating the t statistic is
$$ t = \dfrac{b} {se(b)} $$
```{r}
# Calculate t
t <- b / (SEb)
t

# For a t distribution with n - 2 degrees of freedom it is
onesided <- pt(t, df = dim(father.son)[1] - 2, lower.tail = FALSE)
p <- onesided * 2
p

# The p-value is 1.121268e-69

# Which is the same as 1.121268e-69
summary(lsrl)$coefficients

# Based on these p-values it can be concluded that the null hypothesis that the son height is not associated with the father height can be rejected with an alpha level of 0.05
```




### (h) CI (10 points)
What is the 95% confidence interval for b?
The formula for calculating the confidence interval is
$$ b \pm t_{n - 2, 1 - \dfrac{\alpha}{2}} se(b) $$
```{r}
# Calculate the confidence interval

# t value for 2 degrees of freedom and alpha 0.05
tvalue <- qt(1 - (0.05 /2), df = dim(father.son)[1] - 2, lower.tail = TRUE)
tvalue

# Which is the same as
tvalue <- qt(0.05 /2, df = dim(father.son)[1] - 2, lower.tail = FALSE)
tvalue

# The variation up and down from t will be
variation <- tvalue * SEb
variation 

# Lower confidence interval
lower <- b - variation
lower

# Upper confidence interval
upper <- b + variation
upper

# The 95% confidence interval goes from 0.4610188 to 0.5671673
```




### (i) Simple Multivariate (5 points)
Here we add a random variable as a covariate to our dataset:
```{r}
# Add a random variable to the dataset
set.seed(1)
father.son$random <- rnorm(dim(father.son)[1])
```
Use the lm() function to predict son height using both father height and the random covariate. Comment on the significance (p-value) of the to predictive variables.
```{r}
# Create new linear regression model
newlinearmodel <- lm(sheight ~ fheight + random, data = father.son)

# Results
summary(newlinearmodel)

# The p-value for the father height is <2e-16 which is statistically significant with an alpha level of 0.05. This means that the data allow to reject the null hypothesis that there is no association between son height and father height after controlling for random.
# The p-value for the random is 0.276 which is not statistically significant with an alpha level of 0.05. This means that the data does not allow to reject the null hypothesis that there is no association between son height and random after controlling for father height.
# This is not surprising as father height is expected to predict son height, but random is not expected to predict son height as it is a random set of numbers
```




### (j) Extra (5 points)
Given a father’s height, we can use a simulation method to construct the 100*( 1 - alpha )% confidence
interval for the mean of his son’s height. First draw 1000 samples each of size 1078 with
replacement from the 1078 pairs of father-son heights, then from each sample fit a linear regression
model by the method of least squares, and compute the estimated mean of son’s height.
What are the mean and standard deviation of these 1000 simulated values?
Sort these 1000 estimated means in ascending order. Denote the 25th largest as h_25 and the 975th
largest as h_975 , which are our estimates of the 0.025 and 0.975 quantiles of the sampling distribution
for the mean of son’s height. Then the 100( 1 - alpha )% confidence interval for the mean of the son’s
height is ( h_25 , h_975 ). Compute the 95% confidence interval for the mean of son’s height if his father
is 72 inches tall.
```{r}
# Empty vector to save the estimated means
values <- vector()

# Loop with 1000 rounds
for (i in 1 : 1000) {
  
  # Create the bootstrapped sample
  bootstrappedsampleindices <- sample(1 : 1078, size = 1078, replace = TRUE)
  bootstrappedsample <- father.son[bootstrappedsampleindices, ]
  
  # Linear regression for this round
  linear <- lm(sheight ~ fheight, data = bootstrappedsample)
  
  # Calculate the predicted mean of son height based on this linear model 
  # y equals intercept plus slope times b
  #y = a + b*x
  intercept <- summary(linear)$coefficients[1,1]
  slope <- summary(linear)$coefficients[2,1]
  estimatedmean <- intercept + slope * mean(bootstrappedsample$fheight)
  
  # Saved the estimated means of each round
  values <- c(values, estimatedmean)
}

# Mean and standard deviation of the 1000 simulated values
mean(values)
sd(values)

# Sort the values in ascending order
sortedvalues <- sort(values, decreasing = FALSE)

# Identify the 25th and 75th
summary(sortedvalues)
h_25 <- summary(sortedvalues)[2]
h_75 <- summary(sortedvalues)[5]

# The estimates are 
h_25
h_75





###### Calculate the values if the father's height is 72
# Empty vector to save the estimated means
values <- vector()

# Loop with 1000 rounds
for (i in 1 : 1000) {
  
  # Create the bootstrapped sample
  bootstrappedsampleindices <- sample(1 : 1078, size = 1078, replace = TRUE)
  bootstrappedsample <- father.son[bootstrappedsampleindices, ]
  
  # Linear regression for this round
  linear <- lm(sheight ~ fheight, data = bootstrappedsample)
  
  # Calculate the predicted mean of son height based on this linear model 
  # y equals intercept plus slope times b
  #y = a + b*x
  intercept <- summary(linear)$coefficients[1,1]
  slope <- summary(linear)$coefficients[2,1]
  estimatedmean <- intercept + slope * 72 ### ONLY CHANGE
  
  # Saved the estimated means of each round
  values <- c(values, estimatedmean)
}

# Mean and standard deviation of the 1000 simulated values
mean(values)
sd(values)

# Sort the values in ascending order
sortedvalues <- sort(values, decreasing = FALSE)

# Identify the 25th and 75th
summary(sortedvalues)
h_25 <- summary(sortedvalues)[2]
h_75 <- summary(sortedvalues)[5]

# The estimates are 
h_25
h_75

# 95% Confidence interval 
# from 
h_25
# to
h_75
```